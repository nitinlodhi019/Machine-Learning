# üîç What is SVM?

Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification and regression tasks, but it's mostly used for classification.

## Core Idea:

SVM tries to find the best boundary (hyperplane) that separates data points of different classes with the maximum margin.


![image](https://github.com/user-attachments/assets/3aa16d90-75b5-4aee-9295-92bddc88a331)


# üß† Why Use SVM?

* Works well for high-dimensional spaces.

* Effective in non-linear classification using kernel tricks.

* Strong theoretical foundation and often yields high accuracy.

* Can handle both linear and non-linear classification.

## üìê How Does SVM Work?

**1. Linear SVM (Linearly Separable Data)**
   
* Imagine two classes of points.

* SVM finds the best line (in 2D) or hyperplane (in higher dimensions) that separates them with the maximum margin.

* Support Vectors are the data points closest to the hyperplane. They ‚Äúsupport‚Äù or define the boundary.


**2. Non-Linear SVM**
   
* What if data is not linearly separable?

* SVM uses a trick called the Kernel Trick to map data into a higher-dimensional space where a linear separation is possible.

# üéØ Kernel Trick

## üß† What is a Kernel in SVM?

A kernel is a mathematical function that transforms the input data into a higher-dimensional space without explicitly computing the new coordinates.
This allows SVM to find a linear separating hyperplane in this new space even if the data is not linearly separable in the original space.

## üëâ Why not just transform data manually?

Because computing transformations for high dimensions is computationally expensive.
Kernels do this implicitly and more efficiently via dot product tricks.

## üé® Intuition Behind Kernels

Imagine data shaped like a circle ‚Äî how do you separate it linearly? You can‚Äôt in 2D. But if you map the data into 3D, it might form a separable pattern (like a cone or cylinder).

A kernel function is what allows you to do this transformation ‚Äî like lifting data into a new world where a straight line is suddenly possible.


![Screenshot 2025-04-17 153037](https://github.com/user-attachments/assets/93bd688c-e8dd-4059-88cd-5eb2b93474a2)


# üî¢ SVM for Regression (SVR)

* SVM can also perform regression using Support Vector Regression (SVR).

* Instead of maximizing margin between classes, SVR tries to fit a function within a margin of tolerance (Œµ).

# ‚úÖ Advantages of SVM

* Works well in high-dimensional spaces.

* Effective in cases where number of dimensions > number of samples.

* Memory efficient ‚Äî uses support vectors only.

* Versatile with different kernel functions.

* Great performance in complex, non-linear decision boundaries.

# ‚ùå Disadvantages of SVM

* Not suitable for large datasets (slow training).

* Not ideal when data contains lots of noise or overlapping classes.

* Requires careful tuning of parameters (like C, gamma) and choice of kernel.

* Less interpretable than models like Decision Trees or Linear Regression.
  

![Screenshot 2025-04-17 153323](https://github.com/user-attachments/assets/b0cd5b89-31ec-40c2-bf3e-e2e242001862)
